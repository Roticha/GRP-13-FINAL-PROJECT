{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Flow â€“ Understanding Sentiments expressed about Apple and Google Products using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Business Understanding\n",
    "\n",
    "### 1.1 Introduction \n",
    "In an era where public opinion on products can shape brand perception, companies increasingly rely on Natural Language Processing (NLP) for real-time customer feedback analysis. This project, applies NLP techniques to classify Twitter sentiment related to Apple and Google products, addressing a real-world need for understanding public sentiment in a rapidly evolving market. By using sentiment polarity classification,it provides actionable insights into customer satisfaction and emerging issues, enabling companies, marketing teams, and decision-makers to make data-driven decisions. These insights help brands like Apple and Google improve products, refine customer support strategies, and optimize marketing efforts based on social media sentiment.\n",
    "\n",
    "\n",
    "### 1.2 Problem Statement\n",
    "The problem is to accurately classify the sentiment of tweets related to __Apple and Google products__. We want to determine whether a tweet expresses a positive, negative, or neutral sentiment. This classification can help companies understand customer satisfaction, identify potential issues, and tailor their responses accordingly.\n",
    "\n",
    "### 1.3 Stakeholders \n",
    "- __The companies__: Apple & Google- Considering these companies are direcly affected by the sentiment, it is important for them to gauge the perception of their products so as to identify the areas of improvement.\n",
    "\n",
    "- __Marketing teams__- this sentiment analysis and model can help them respond to negative feedback, adjust their marketing campaigns and highlight the positive aspects of their products.\n",
    "\n",
    "- __The customer support teams &decision makers__- the sentiment analysis is important for they can use it to improve product development, customer support and brand reputation.\n",
    "\n",
    "### 1.4  Business Value\n",
    "By accurately classifying tweets, our NLP model provides actionable insights to stakeholders. For example:\n",
    "\n",
    "- Identifying negative sentiment can help companies address issues promptly.\n",
    "- Recognizing positive sentiment can guide marketing efforts and reinforce successful strategies.\n",
    "- Understanding neutral sentiment can provide context and balance.\n",
    "\n",
    "### 1.5 Objectives \n",
    "Main Objective\n",
    "\n",
    "To develop a NLP (Natural Language Processing) multiclass classification model for sentiment analysis, aim to achieve a __recall score of 80%__ and an __accuracy of 80%__. The model should categorize sentiments into three classes: __Positive__, __Negative__, and __Neutral__.\n",
    "\n",
    "Specific Objectives\n",
    "\n",
    "- To idenitfy the most common words used in the dataset using Word cloud.\n",
    "\n",
    "- To confirm the most common words that are positively and negatively tagged.\n",
    "\n",
    "- To recognize the products that have been opined by the users.\n",
    "\n",
    "- To spot the distribution of the sentiments.\n",
    "\n",
    "### 1.6 Conclusion \n",
    "Our NLP model will contribute valuable insights to the real-world problem of understanding Twitter sentiment about Apple and Google products. Stakeholders can leverage this information to enhance their decision-making processes and improve overall customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n",
    "\n",
    "### 2.1 Data source\n",
    "The dataset originates from __CrowdFlower via data.world__. Contributors evaluated tweets related to various brands and products. Specifically:\n",
    "\n",
    "- Each tweet was labeled as expressing __positive__, __negative__, __no emotion__ or __can't tell__ toward a brand or product.\n",
    "- If emotion was expressed, contributors specified which brand or product was the target.\n",
    "\n",
    "### 2.2 Suitability of the Data\n",
    "Here's why this dataset is suitable for our project:\n",
    "\n",
    "- __Relevance__: The data directly aligns with our business problem of understanding Twitter sentiment for Apple and Google products.\n",
    "- __Real-World Context__: The tweets represent actual user opinions, making the problem relevant in practice.\n",
    "- __Multiclass Labels__: We can build both binary (positive/negative) and multiclass (positive/negative/neutral) classifiers using this data.\n",
    "\n",
    "### 2.3 Dataset Size\n",
    "The dataset contains __over 9,000 labeled tweets__. We'll explore its features to gain insights.\n",
    "\n",
    "### 2.4 Descriptive Statistics\n",
    "- __tweet_text__: The content of each tweet.\n",
    "- __is_there_an_emotion_directed_at_a_brand_or_product__: No emotion toward brand or product, Positive emotion, Negative emotion, I can't tell\n",
    "- __emotion_in_tweet_is_directed_at__: The brand or product mentioned in the tweet.\n",
    "\n",
    "### 2.5 Feature Selection\n",
    "__Tweet text__ is the primary feature. The emotion label and target brand/product are essential for classification.\n",
    "\n",
    "### 2.6 Data Limitations\n",
    "- __Label Noise__: Human raters' subjectivity may introduce noise.\n",
    "- __Imbalanced Classes__: We'll address class imbalance during modeling.\n",
    "- __Contextual Challenges__: Tweets are often short and context-dependent.\n",
    "- __Incomplete & Missing Data__: Could affect the overall performance of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "### 3.1 Importing Necessary Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# wordCloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# pickle\n",
    "import pickle\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading dataset\n",
    "\n",
    "file_path = r\"C:\\Users\\PC\\Documents\\Flatiron\\dsc-data-science-env-config\\Phase_5_capstone_project\\judge_tweet_product_company.csv\"\n",
    "# Our dataset contains special characters or a non-standard encoding.\n",
    "# We solved this by reading the file using different encoding \"ISO-8859-1\"\n",
    "data= pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\n",
      "----\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Checking data information\n",
    "print(\"INFO\")\n",
    "print(\"-\" * 4)\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (9093, 3)\n",
      "Number of Rows: 9093\n",
      "Number of Columns: 3\n"
     ]
    }
   ],
   "source": [
    "# Find the shape of the DataFrame\n",
    "data_shape = data.shape\n",
    "\n",
    "# Print the shape\n",
    "print(\"Data Shape:\", data_shape)\n",
    "print(\"Number of Rows:\", data_shape[0])\n",
    "print(\"Number of Columns:\", data_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UNIQUE VALUES\n",
      "------------\n",
      "Column *tweet_text* has 9065 unique values\n",
      "\n",
      "Column *emotion_in_tweet_is_directed_at* has 9 unique values\n",
      "Top unique values in the *emotion_in_tweet_is_directed_at* include:\n",
      "- iPad\n",
      "- Apple\n",
      "- iPad or iPhone App\n",
      "- Google\n",
      "- iPhone\n",
      "- Other Google product or service\n",
      "- Android App\n",
      "- Android\n",
      "- Other Apple product or service\n",
      "\n",
      "Column *is_there_an_emotion_directed_at_a_brand_or_product* has 4 unique values\n",
      "Top unique values in the *is_there_an_emotion_directed_at_a_brand_or_product* include:\n",
      "- No emotion toward brand or product\n",
      "- Positive emotion\n",
      "- Negative emotion\n",
      "- I can't tell\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unique Values\n",
    "print(\"\\n\\nUNIQUE VALUES\")\n",
    "print(\"-\" * 12)\n",
    "for col in data.columns:\n",
    "    print(f\"Column *{col}* has {data[col].nunique()} unique values\")\n",
    "    if data[col].nunique() < 12:\n",
    "        print(f\"Top unique values in the *{col}* include:\")\n",
    "        for idx in data[col].value_counts().index:\n",
    "            print(f\"- {idx}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MISSING VALUES\n",
      "---------------\n",
      "Column *tweet_text* has 1 missing values.\n",
      "Column *emotion_in_tweet_is_directed_at* has 5802 missing values.\n",
      "Column *is_there_an_emotion_directed_at_a_brand_or_product* has 0 missing values.\n"
     ]
    }
   ],
   "source": [
    "# Missing or Null Values\n",
    "print(\"\\nMISSING VALUES\")\n",
    "print(\"-\" * 15)\n",
    "for col in data.columns:\n",
    "    print(f\"Column *{col}* has {data[col].isnull().sum()} missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate Values\n",
    "print(\"\\n\\nDUPLICATE VALUES\")\n",
    "print(\"-\" * 16)\n",
    "print(f\"The dataset has {data.duplicated().sum()} duplicated records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "1. All the columns are in the correct data types.\n",
    "\n",
    "2. The columns will need to be renamed.\n",
    "\n",
    "3. Features with missing values should be renamed from NaN.\n",
    "\n",
    "4. Duplicate records should be dropped.\n",
    "\n",
    "5. All records with the target as \"I can't tell\" should be dropped.\n",
    "\n",
    "6. Corrupted records should be removed.\n",
    "\n",
    "7. Rename values in the is_there_an_emotion_directed_at_a_brand_or_product where the value is 'No emotion toward brand or product' to 'Neutral Emotion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Data Cleaning & Feature Engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
